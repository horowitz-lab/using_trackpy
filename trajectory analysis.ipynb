{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trackpy as tp\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.lines as mlines\n",
    "import pickle\n",
    "import cv2\n",
    "import math\n",
    "from datetime import date\n",
    "import datetime\n",
    "import time\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "import scipy.optimize as sco\n",
    "import seaborn as sns\n",
    "import av\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import winsound\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import warnings\n",
    "from scipy.signal import savgol_filter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "display(HTML(\"<style>.container { width: 100% !important; }</style>\")) # Set the custom CSS to make the notebook width 100% of the window size\n",
    "\n",
    "\n",
    "def find_analysis_folders(root_folder):\n",
    "    analysis_folders = []\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for folder in dirs:\n",
    "            if folder.lower() == \"analysis\":\n",
    "                analysis_folders.append(os.path.join(root, folder))\n",
    "    return analysis_folders\n",
    "\n",
    "\n",
    "def get_subfolders(analysis_folders):\n",
    "    subfolders = []\n",
    "    for folder in analysis_folders:\n",
    "        for subitem in os.listdir(folder): # not recursive; just go one level deeper.\n",
    "            subitem_path = os.path.join(folder, subitem)\n",
    "            if os.path.isdir(subitem_path):\n",
    "                subfolders.append(subitem_path)\n",
    "    return subfolders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c269770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder_list = False\n",
    "\n",
    "\n",
    "# avi_folder = r'C:\\Users\\vhorowit\\Desktop\\BessLawrence\\Data'\n",
    "avi_folder = r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Data'\n",
    "#blocking = 'Bess'\n",
    "blocking = 'Rebecca and Alex'\n",
    "AlexRebecca = True\n",
    "\n",
    "#info_csv = r'G:\\Shared drives\\Horowitz Lab Notes\\Horowitz, Viva - notes and files\\subdiffusion 2023\\2023 Subdiffusion project data - Sheet1.csv'\n",
    "info_csv = r'C:\\Users\\vhorowit\\Desktop\\2023 Subdiffusion project data - Sheet1.csv'\n",
    "\n",
    "#r'G:\\Shared drives\\Horowitz Lab Notes\\Bess Lawrence Data'\n",
    "\n",
    "if create_folder_list:\n",
    "    analysis_folders = find_analysis_folders(r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Analysis')\n",
    "    folderlist = get_subfolders(analysis_folders)\n",
    "else:\n",
    "    #folderlist = [r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Analysis\\2023-05-28',\n",
    "    #             r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Analysis\\2023-05-27']\n",
    "    \n",
    "    ## offline access on Viva's laptop.\n",
    "    folderlist = [r'C:\\Users\\vhorowit\\Desktop\\Rebecca and Alex Analysis\\2023-05-27',\n",
    "                 r'C:\\Users\\vhorowit\\Desktop\\Rebecca and Alex Analysis\\2023-05-28']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8051a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_csv(info_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c937bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_info_df = info_df[info_df['Blocking'] == blocking]\n",
    "this_info_df['color'] = this_info_df['color'].str.upper()\n",
    "\n",
    "display(this_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a43e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folderlist = [r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Analysis\\2023-05-27',\n",
    "#             r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Analysis\\2023-05-28']\n",
    "\n",
    "##savefolder = r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton\\Analysis\\viva-analysis'\n",
    "savefolder = r'C:\\Users\\vhorowit\\Desktop\\Rebecca and Alex Analysis\\2024-viva-analysis' ## offline access\n",
    "#savefolder = r'C:\\Users\\vhorowit\\Desktop\\BessLawrence\\Viva-Analysis'\n",
    "saving = False\n",
    "\n",
    "subtract_drift_slow = False\n",
    "calculate_van_hove_slow = False\n",
    "\n",
    "# scaling, measured in microns per pixel\n",
    "#scaling = 330 / 1247.96 # 20x1.0, measured 2021-06-17\n",
    "#scaling = 220 / 1250.04 # 20x1.5, measured 2021-06-17\n",
    "scaling = 150 / 1127.54 # 40x1.0, measured 2021-06-16\n",
    "#scaling = 100 / 1130.61 # 40x1.5, measured 2021-06-16\n",
    "#scaling = 80 / 914.92 # 60x1.0, measured 2021-05-28\n",
    "#scaling = 60 / 1031.07 # 60x1.5, measured 2021-05-28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datestring():\n",
    "    return datetime.datetime.today().strftime('%Y-%m-%d %H;%M;%S')\n",
    "\n",
    "datestr = datestring()\n",
    "\n",
    "#remove parentheses and numbers from filenames\n",
    "def remove_number_parentheses(filename):\n",
    "    return re.sub(r'\\(\\d\\)', '', filename)\n",
    "\n",
    "def savefigure(savename):\n",
    "    try:\n",
    "        plt.savefig(savename + '.svg', dpi = 600, bbox_inches='tight', transparent=True)\n",
    "    except:\n",
    "        print('Could not save svg')\n",
    "    try:\n",
    "        plt.savefig(savename + '.pdf', dpi = 600, bbox_inches='tight', transparent=True)\n",
    "           # transparent true source: https://jonathansoma.com/lede/data-studio/matplotlib/exporting-from-matplotlib-to-open-in-adobe-illustrator/\n",
    "    except:\n",
    "        print('Could not save pdf')\n",
    "    plt.savefig(savename + '.png', dpi = 600, bbox_inches='tight', transparent=True)\n",
    "    print(\"Saved:\\n\", savename + '.png')\n",
    "\n",
    "def beep():\n",
    "    try:\n",
    "        winsound.PlaySound(r'C:\\Windows\\Media\\Speech Disambiguation.wav', flags = winsound.SND_ASYNC)\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        winsound.PlaySound(\"SystemHand\", winsound.SND_ALIAS)\n",
    "        return\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        winsound.Beep(450,150)\n",
    "        return\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fc915",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "filename_list = []\n",
    "\n",
    "\"\"\"\n",
    "Note about file names\n",
    "unfiltered.pkl = every particle trajectory, immediately after it was linked.\n",
    ".pkl = filtered to remove stubs\n",
    "nodrift.pkl = drift has been subtracted\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for folder in folderlist:\n",
    "    os.chdir(folder)\n",
    "    # Iterate over files in the folder\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('control_tracer.pkl'):\n",
    "            #print(os.path.join(folder,filename))\n",
    "            if os.path.isfile(filename):\n",
    "                #df_list.append(pd.read_pickle(filename))\n",
    "                if verbose:\n",
    "                    print(filename)\n",
    "                filename_list.append(os.path.join(folder,filename)) ## build list of every control_tracer.pkl (to be overwritten)\n",
    "            else:\n",
    "                print('is not a file. **************')\n",
    "   \n",
    "print('Extract short files')\n",
    "def extract_short_file(filepath):\n",
    "    match = re.search(r'Data Taken (\\d{4}-\\d{2}-\\d{2}), (.*)control_tracer.pkl', filepath)\n",
    "    if match:\n",
    "        extracted_part = match.group(2).strip()\n",
    "    else:\n",
    "        base_name = os.path.basename(filepath)  # Get the base name from the path\n",
    "        start_index = base_name.rfind(',') + 2  # Find the index of the comma and add 2 to skip the comma and space\n",
    "        extracted_part = base_name[start_index:].replace(\"control_tracer.pkl\", \"\").strip()  # Remove \"control_tracer.pkl\" and leading/trailing spaces\n",
    "    return extracted_part\n",
    "\n",
    "shortfiles = [extract_short_file(item) for item in filename_list]\n",
    "\n",
    "print('Create analysis_files_df with list of all the saved trajectories in the folders.')\n",
    "## the analysis_files from the folders might includes trajectories that we are excluding, and will certainly be in a different order than this_info_df.\n",
    "analysis_files_df = pd.DataFrame({'Analysis file': filename_list, 'control_tracer.pkl': shortfiles }) ## list with every control_tracer.pkl\n",
    "\n",
    "combined_df = analysis_files_df.merge(this_info_df, on='control_tracer.pkl') ## merge list of control_tracer.pkl with existing spreadsheet\n",
    "\n",
    "print('Read in all the trajectory files')\n",
    "df_list = []\n",
    "filename_list = list(combined_df['Analysis file'])   # overwrite list of control_tracer.pkl with list from spreadsheet\n",
    "assert (filename_list == list(combined_df['Analysis file']))\n",
    "for filepath in filename_list:\n",
    "    df_list.append(pd.read_pickle(filepath)) # read trajectory files\n",
    "assert (len(df_list) == len(combined_df))\n",
    "combined_df['trajectories_raw'] = df_list\n",
    "  \n",
    "    \n",
    "access_datafiles = False\n",
    "if access_datafiles:\n",
    "    print('Get fps from avi')\n",
    "    def get_fps_from_avi(avi_file): # avi_file is a file path\n",
    "        assert(os.path.exists(avi_file))\n",
    "        try:\n",
    "            video = cv2.VideoCapture(avi_file)\n",
    "            fps = video.get(cv2.CAP_PROP_FPS)\n",
    "            video.release()\n",
    "            if fps == 0:\n",
    "                raise Exception(f\"fps must not be zero: {avi_file}\")\n",
    "        except:\n",
    "            print('Using backup method')\n",
    "            import av\n",
    "            container = av.open(avi_file)\n",
    "            fps = container.streams.video[0].average_rate\n",
    "            container.close()\n",
    "        return float(fps)\n",
    "\n",
    "    data_files = []\n",
    "    fps_list = []\n",
    "    #os.chdir(r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton')\n",
    "    #for filename in os.listdir(r'G:\\Shared drives\\Horowitz Lab Notes\\Rebecca Dalphin and Alex Axton'):\n",
    "    for root, dirs, files in os.walk(avi_folder):\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(root, filename)\n",
    "            if filename.endswith('.avi'):\n",
    "                #print(full_path)\n",
    "                fps = get_fps_from_avi(full_path)\n",
    "                data_files.append(full_path)\n",
    "                fps_list.append(fps)\n",
    "                #print(fps)\n",
    "                \n",
    "    datafiles_df = pd.DataFrame({\"Data file\":data_files, \"fps\": fps_list })\n",
    "\n",
    "    datafiles_df['data_file name'] = datafiles_df['Data file'].apply(lambda x: x.split('\\\\')[-1])\n",
    "    \n",
    "    combined_df1 = combined_df.copy()\n",
    "    combined_df = combined_df.merge(datafiles_df, on='data_file name')\n",
    "else:\n",
    "    print('Placeholder: Just set fps to 24.404 for everything.')\n",
    "    fps_list = [24.404 for file in list(combined_df['Analysis file'])]\n",
    "    combined_df['fps'] = fps_list\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"with pd.option_context('display.max_rows', None, \n",
    "                       'display.max_columns', None, \n",
    "                       'display.max_colwidth', None, \n",
    "                       'display.expand_frame_repr', False):\n",
    "    display(datafiles_df)\"\"\";\n",
    "\n",
    "assert (filename_list == list(combined_df['Analysis file'])) \n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da496b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(df_list) == len(combined_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(combined_df[['data_file name', 'control_tracer.pkl', 'fps']]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8e91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62030ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, \n",
    "                       'display.max_columns', None, \n",
    "                       'display.max_colwidth', None, \n",
    "                       'display.expand_frame_repr', False):\n",
    "    display(combined_df[['power n','data_file name', 'Analysis file']] )   ###  do these match? Looks ok. The PEG 40% movies are named poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf58a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071118cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datafiles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7509cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0de600",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(this_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1d2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(analysis_files_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf58be",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_info_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(savefolder)\n",
    "showgraphs_slow = True    # 32.4 minutes for Rebecca and Alex's data with graphing)\n",
    "\n",
    "start = time.time() # about 3 minutes \n",
    "\n",
    "if subtract_drift_slow:\n",
    "    from rotational_drift_subtraction import drift_subtract\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n",
    "    # Some day this will come back to bite me. Today is not that day.\n",
    "   \n",
    "\n",
    "    df_list_nodrift = []\n",
    "    for df in combined_df['trajectories_raw']: #tqdm_notebook(combined_df['trajectories_raw']):\n",
    "        if showgraphs_slow:\n",
    "            tp.plot_traj(df, mpp = scaling)\n",
    "        _, df_nodrift = drift_subtract(tracer = df, show_plots=False ) # here, nodrift means the drift has been subtracted.\n",
    "        df_list_nodrift.append(df_nodrift)\n",
    "\n",
    "\n",
    "    # Re-enable the warning\n",
    "    warnings.filterwarnings(\"once\", category=PendingDeprecationWarning)\n",
    "    \n",
    "    data_dict = {'filename_list': filename_list, 'df_list_nodrift':df_list_nodrift}\n",
    "    with open('drift_subtracted_trajectories_dictionary.pkl', 'wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "    beep()\n",
    "else:\n",
    "    with open('drift_subtracted_trajectories_dictionary.pkl', 'rb') as file:\n",
    "        data_dict = pickle.load(file)\n",
    "    print('Opened file:', str(os.getcwd()), '\\n', 'drift_subtracted_trajectories_dictionary.pkl')\n",
    "    # Extract elements from the dictionary\n",
    "    filename_list2 = data_dict['filename_list']\n",
    "    assert (filename_list == filename_list2)\n",
    "    df_list_nodrift = data_dict['df_list_nodrift'] \n",
    "    \n",
    "combined_df['trajectories_full_subtracted'] = df_list_nodrift\n",
    "    \n",
    "end = time.time()\n",
    "print((end-start)/60, 'minutes elapsed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(combined_df.loc[:,'Analysis file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e354b60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#video = 5\n",
    "\n",
    "#or video in range(len(combined_df.loc[:,'Analysis file'])):\n",
    "for video in range(5,len(combined_df.loc[:,'Analysis file'])):\n",
    "\n",
    "    print(combined_df.loc[video,'Analysis file'])\n",
    "\n",
    "    tp.plot_traj(combined_df.loc[video,'trajectories_raw'], mpp=scaling)\n",
    "    tp.plot_traj(combined_df.loc[video,'trajectories_full_subtracted'], mpp=scaling);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501dbb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537fe996",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(df_list) == len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display trajectory df for any video\n",
    "\n",
    "index = 2\n",
    "display(filename_list[index])\n",
    "display(df_list_nodrift[index])\n",
    "display(combined_df.trajectories_full_subtracted[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55021160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_nodrift = pd.DataFrame(data_dict)['df_list_nodrift']\n",
    "\n",
    "assert len(df_series_nodrift) == len(df_list_nodrift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "skipfirst = False\n",
    "\n",
    "if skipfirst:\n",
    "    ## Skip the first frame of each movie: it's the most likely to have dropped frames.\n",
    "    df_list_nodrift = [df_series_nodrift[video][df_series_nodrift[video].frame > 1] for video in range(len(df_series_nodrift))]\n",
    "combined_df['trajectories_subtracted'] = df_list_nodrift # truncated\n",
    "\n",
    "if debug and len(filename_list) == len(df_list_nodrift):  # Just work with a few of the movies.\n",
    "    combined_df_original = combined_df\n",
    "    del combined_df\n",
    "    combined_df = combined_df_original[10:15].copy()\n",
    "    df_list_nodrift_orig = df_list_nodrift.copy()\n",
    "    df_list_nodrift = df_list_nodrift[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17095820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalizes histogram data so that the sum of probabilities is one.\n",
    "\n",
    "@param histdata - the starting histogram\n",
    "\n",
    "@return the normalized histogram of probabilites\n",
    "\"\"\"\n",
    "def manualnorm(histdata):\n",
    "     return (1/(histdata.sum()*binwidth))*histdata\n",
    "    \n",
    "\"\"\"\n",
    "Outputs f(x) where f is a Gaussian curve.\n",
    "\n",
    "@param x - the independent variable\n",
    "@param a [0] - Gaussian amplitude\n",
    "@param center [1] - Gaussian center\n",
    "@param sigma [2] - Gaussian standard deviation (width)\n",
    "\n",
    "@return f(x)\n",
    "\"\"\"\n",
    "def gaussian(x,a,center,sigma):\n",
    "    return a*(np.exp(-((x-center)**2)/(2*(sigma**2)))) # sigma is standard deviation.\n",
    "\n",
    "height_index = 0\n",
    "center_index = 1\n",
    "sigma_index = 2 # after x, the parameters are 0: a, 1: center, 2: sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shortfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb765e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_list_nodrift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8828ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortfiles == combined_df['control_tracer.pkl']  # all true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fca8f",
   "metadata": {},
   "source": [
    "# Mean square displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_fresh_emsd_slow = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fefa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set values\n",
    "emsd_max_lagtime = 100\n",
    "saving = True\n",
    "\n",
    "calc_fresh_emsd_slow = True    # True: 7 or 8 minutes. False: 3 seconds.\n",
    "\n",
    "start = time.time() \n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "# Create empty lists to store handles and labels\n",
    "handles = []\n",
    "labels = []\n",
    "label_colors = {}\n",
    "\n",
    "\n",
    "## Iterate through files and calculate emsd\n",
    "\n",
    "# tp.imsd(data, mpp=scaling, fps=fps, max_lagtime=1000)\n",
    "# tp.emsd(data, mpp=scaling, fps=fps, max_lagtime = emsd_max_lagtime).replace(0, np.nan).dropna()\n",
    "\n",
    "## I'm getting this warning for trackpy but that is not my problem.\n",
    "# Warning message:\n",
    "# Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. \n",
    "# Use groupby instead. df.median(level=1) should use df.groupby(level=1).median().\n",
    "# https://github.com/soft-matter/trackpy/issues/738\n",
    "#assert len(df_series_nodrift) == len(combined_df)\n",
    "#assert len(df_list_nodrift) == len(combined_df)\n",
    "\n",
    "if calc_fresh_emsd_slow: # Calculate eMSD\n",
    "    emsd_list = []\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    for (_, file_info) in combined_df.iterrows():\n",
    "        this_emsd = tp.emsd(\n",
    "            file_info.trajectories_subtracted, \n",
    "            mpp=scaling, \n",
    "            fps=file_info.fps, \n",
    "            max_lagtime = emsd_max_lagtime).replace(0, np.nan).dropna()\n",
    "        emsd_list.append(this_emsd)\n",
    "\n",
    "    warnings.filterwarnings(\"once\", category=FutureWarning)\n",
    "    \n",
    "    #Save emsd_list \n",
    "    if skipfirst:\n",
    "        string = ',frame_1_removed'\n",
    "    else:\n",
    "        string = ',frame_1_included'\n",
    "    datestr = datestring()\n",
    "    emsd_file = datestr + 'emsd_file' + string + '.pkl' \n",
    "    \n",
    "    with open(emsd_file, 'wb') as file:\n",
    "        pickle.dump(emsd_list, file)\n",
    "    print('Saved:', emsd_file)            \n",
    "\n",
    "else: # Load eMSD file\n",
    "    file_to_open = '2023-08-10 20;57;34emsd_file,frame_1_included.pkl'\n",
    "    with open(file_to_open, 'rb') as file:\n",
    "        emsd_list = pickle.load(file)\n",
    "    print('Loaded:',file_to_open )\n",
    "    \n",
    "combined_df['emsd'] = emsd_list\n",
    "\n",
    "fit_A_values = []  # List to store this_fit.A[0] values\n",
    "fit_n_values = []  # List to store this_fit.n[0] values    \n",
    "    \n",
    "# Plot and curve-fit eMSD\n",
    "for (_, file_info) in combined_df.iterrows():\n",
    "    this_emsd = file_info.emsd\n",
    "    #drift_subtracted_traj_df = file_info.trajectories_full_subtracted\n",
    "    \n",
    "    (file_info.emsd).plot(loglog=True, figsize = [3,3], style = '.', color = file_info.color,  grid=False, alpha = .3, ax = ax )\n",
    "    this_fit = tp.utils.fit_powerlaw(file_info.emsd, plot = False)\n",
    "    ## Just plot top and bottom point, since it's a straight line\n",
    "    this_fit_x = [1/fps, (1/fps) * len(file_info.emsd)]\n",
    "    this_fit_y = [this_fit.A[0] * np.power(this_fit_x[0], this_fit.n[0]),\n",
    "                         this_fit.A[0] * np.power(this_fit_x[1], this_fit.n[0])]\n",
    "    plt.plot(this_fit_x, this_fit_y, color= file_info.color, alpha = .3)\n",
    "\n",
    "    # Store this_fit.A[0] and this_fit.n[0] values\n",
    "    fit_A_values.append(this_fit.A[0])\n",
    "    fit_n_values.append(this_fit.n[0])\n",
    "\n",
    "    # Append the handle and label to the lists\n",
    "    line_handle, _ = plt.gca().get_legend_handles_labels()\n",
    "    handles.extend(line_handle)\n",
    "    labels.append(file_info.Description)\n",
    "\n",
    "    # Map label to color\n",
    "    if file_info.Description not in label_colors:\n",
    "        label_colors[file_info.Description] = file_info.color\n",
    "\n",
    "    \n",
    "\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('eMSD ($\\mu$m$^2$)')\n",
    "plt.axis('equal')\n",
    "\n",
    "# Add fit_A_values and fit_n_values as columns to combined_df\n",
    "combined_df['coeff A [um^2/s]'] = fit_A_values\n",
    "combined_df['alpha'] = fit_n_values\n",
    "\n",
    "# Consolidate the figure legend to just the unique labels\n",
    "new_handles = []\n",
    "new_labels = []\n",
    "handle_mapping = {}\n",
    "for handle, label in zip(handles, labels):\n",
    "    if label not in handle_mapping:\n",
    "        handle_mapping[label] = handle\n",
    "        new_handles.append(handle)\n",
    "        new_labels.append(label)\n",
    "\n",
    "\n",
    "# Set the modified handles and labels in the legend\n",
    "legend = ax.legend(handles=new_handles, labels=new_labels, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "# Set the colors of legend lines based on label_colors dictionary\n",
    "for line, label in zip(legend.get_lines(), new_labels):\n",
    "    line.set_color(label_colors[label])\n",
    "\n",
    "# Set the alpha value of legend lines to 1.0 (no transparency)\n",
    "for line in legend.get_lines():\n",
    "    line.set_alpha(1.0)\n",
    "\n",
    "\n",
    "if saving:\n",
    "    datestr = datestring()\n",
    "    savefigure(datestr + '_eMSD')\n",
    "    \n",
    "end = time.time()\n",
    "print('Elapsed: ', (end-start)/60, 'minutes')\n",
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba24582",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, \n",
    "                       'display.max_columns', None, \n",
    "                       'display.max_colwidth', None, \n",
    "                       'display.expand_frame_repr', False):\n",
    "    display(combined_df[['alpha','power n','data_file name', 'Analysis file']] )   ### *** do these match?\n",
    "    # Most do, but three don't.\n",
    "    # Line 13 is especially bad. but that is 8000  PEG @ 25mg per mL (1), which we don't want to consider anyway. (How did Rebecca and Alex make it seem acceptable??)\n",
    "    # line 14 and 15 don't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d95276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"this_emsd = tp.emsd(\n",
    "        drift_subtracted_traj_df, \n",
    "        mpp=scaling, \n",
    "        fps=file_info.fps, \n",
    "        max_lagtime = emsd_max_lagtime).replace(0, np.nan).dropna()\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94249a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this_emsd.plot(loglog=True, figsize = [3,3], style = '.', color = file_info.color,  grid=False, alpha = 1,  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f44db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_emsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddfcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this_fit = tp.utils.fit_powerlaw(this_emsd, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db549fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info['Analysis file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665f173",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots(1, 1)\n",
    "\n",
    "\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'alpha'\n",
    "plt.plot(combined_df[xlabel], combined_df[ylabel], '.' )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fee4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots(1, 1)\n",
    "\n",
    "\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'alpha'\n",
    "plt.plot(combined_df[xlabel], combined_df[ylabel], '.' )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30633c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipynb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed13f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipynb_df = pd.read_pickle('C:\\\\Users\\\\vhorowit\\\\Desktop\\\\Rebecca Dalphin and Alex Axton\\\\ipynb summary.pkl') # I created this file by reading every ipynb from Rebecca and Alex.\n",
    "\n",
    "# Convert both columns to lowercase before merging\n",
    "ipynb_df['match_column'] = ipynb_df['ipynb file'].str.lower()\n",
    "ipynb_df['match_column'] = ipynb_df['match_column'].str.replace('200 peg and 2000 peg', '200 and 2000 peg')\n",
    "ipynb_df['match_column'] =  ipynb_df['match_column'].str.replace('8000 peg and 20000 peg', '8000 and 20000 peg')\n",
    "ipynb_df['match_column'] =  ipynb_df['match_column'].str.replace('8000 peg @ 25mg per ml', '8000 peg @ 25mg per ml')\n",
    "ipynb_df['match_column'] = ipynb_df['match_column'].str.strip()\n",
    "combined_df['match_column'] = combined_df['control_tracer.pkl'].str.lower().str.replace('  ', ' ')\n",
    "combined_df['match_column'] = combined_df['match_column'].str.strip()\n",
    "\n",
    "merged_df = pd.merge(ipynb_df, combined_df, on='match_column', how='outer', indicator=True)\n",
    "\n",
    "display(merged_df[['control_tracer.pkl', 'n', 'A', 'power n', 'coef A', 'alpha', '_merge']])\n",
    "#merged_df[ 'n'] - merged_df['power n'] ## no difference\n",
    "\n",
    "\n",
    "## *** to do:\n",
    "## Figure out why powers aren't the same numbers.\n",
    "## n is from the ipynb\n",
    "## power n is from Rebecca and Alex's spreadsheet. It matches the ipynb! No, it doesn't??\n",
    "## alpha is my own calculations here. it does match n from the ipynb.\n",
    "\n",
    "### 2023-08-10 I fixed the spreadsheet file\n",
    "### 2024-01-06 I fixed the list order errors in this ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6ce70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ipynb_df = pd.read_pickle('C:\\\\Users\\\\vhorowit\\\\Desktop\\\\Rebecca Dalphin and Alex Axton\\\\ipynb summary.pkl') # I created this file by reading every ipynb from Rebecca and Alex.\n",
    "\n",
    "# Convert both columns to lowercase before merging\n",
    "ipynb_df['match_column'] = ipynb_df['ipynb file'].str.lower()\n",
    "ipynb_df['match_column'] = ipynb_df['match_column'].str.replace('200 peg and 2000 peg', '200 and 2000 peg')\n",
    "ipynb_df['match_column'] =  ipynb_df['match_column'].str.replace('8000 peg and 20000 peg', '8000 and 20000 peg')\n",
    "ipynb_df['match_column'] =  ipynb_df['match_column'].str.replace('8000 peg @ 25mg per ml', '8000 peg @ 25mg per ml')\n",
    "ipynb_df['match_column'] = ipynb_df['match_column'].str.strip()\n",
    "combined_df['match_column'] = combined_df['control_tracer.pkl'].str.lower().str.replace('  ', ' ')\n",
    "combined_df['match_column'] = combined_df['match_column'].str.strip()\n",
    "\n",
    "merged_df = pd.merge(ipynb_df, combined_df, on='match_column', how='outer', indicator=True)\n",
    "\n",
    "display(merged_df[['control_tracer.pkl', 'n', 'A', 'power n', 'coef A', 'alpha', '_merge']])\n",
    "#merged_df[ 'n'] - merged_df['power n'] ## no difference\n",
    "\n",
    "\n",
    "## *** to do:\n",
    "## Figure out why powers aren't the same numbers.\n",
    "## n is from the ipynb\n",
    "## power n is from Rebecca and Alex's spreadsheet. It matches the ipynb!\n",
    "## alpha is my own calculations here. it does NOT match.\n",
    "\n",
    "### 2023-08-10 I just fixed the spreadsheet file and I need to make sure to reload the correct csv\n",
    "\n",
    "#### previous output!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((merged_df[merged_df['_merge'] == 'both']['n']) , 'o', label='student spreadsheet')\n",
    "plt.plot((merged_df[merged_df['_merge'] == 'both']['power n']) , label='student ipynb')\n",
    "plt.plot((merged_df[merged_df['_merge'] == 'both']['alpha']), '.',label = 'VRH curvefit of student trajectory')\n",
    "plt.legend()\n",
    "plt.ylabel('power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((merged_df[merged_df['_merge'] == 'both']['A']), 'o')\n",
    "plt.plot((merged_df[merged_df['_merge'] == 'both']['coef A']), '.')\n",
    "plt.ylabel('coef A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab24aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, \n",
    "                       'display.max_columns', None, \n",
    "                       'display.max_colwidth', None, \n",
    "                       'display.expand_frame_repr', False):\n",
    "    display(merged_df[['alpha','power n', 'n' ,'data_file name', 'Analysis file', 'ipynb file',]].iloc[15] )   # do these match? yes, except 13, 14, 15: power n is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, \n",
    "                       'display.max_columns', None, \n",
    "                       'display.max_colwidth', None, \n",
    "                       'display.expand_frame_repr', False):\n",
    "    display(merged_df[['alpha','power n','data_file name', 'Analysis file', 'ipynb file',]] )   ### *** do these match?\n",
    "    \n",
    "    ## not 13, 14, 15.\n",
    "    ## lines 2 and 3 are duplicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a40b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba64e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['kurtosis (15 frames)','excess kurtosis (from cell 47) (frame 15)' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c04167",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea51b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.iloc[:,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c03e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the 'color' column and update NaN values with 'C3'\n",
    "for index, c in enumerate(merged_df['color']):\n",
    "    if pd.isnull(c):\n",
    "        merged_df.loc[index, 'color'] = 'C3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466399d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax1 = plt.subplots(1, 1, figsize = [3,3])\n",
    "\n",
    "\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'power n'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "\n",
    "fig0, ax3 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'alpha'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "fig0, ax4 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'n'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "\n",
    "fig0, ax2 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'PEG 20000 concentration'\n",
    "ylabel = 'power n'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color ) \n",
    "plt.xlabel(xlabel+ ' (mg/mL)')\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "\n",
    "if 'new_handles' in dir():\n",
    "    for ax in [ax1, ax2]:\n",
    "        # Set the modified handles and labels in the legend\n",
    "        legend = ax.legend(handles=new_handles, labels=new_labels, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "        # Set the colors of legend lines based on label_colors dictionary\n",
    "        for line, label in zip(legend.get_lines(), new_labels):\n",
    "            line.set_color(label_colors[label])\n",
    "\n",
    "        # Set the alpha value of legend lines to 1.0 (no transparency)\n",
    "        for line in legend.get_lines():\n",
    "            line.set_alpha(1.0)\n",
    "\n",
    "### new output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax1 = plt.subplots(1, 1, figsize = [3,3])\n",
    "\n",
    "\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'power n'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "\n",
    "fig0, ax3 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'alpha'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "fig0, ax4 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'avg PEG molec mass (g/mol)'\n",
    "ylabel = 'n'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color )\n",
    "plt.xlabel(xlabel)\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "\n",
    "fig0, ax2 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'PEG 20000 concentration'\n",
    "ylabel = 'power n'\n",
    "plt.scatter(merged_df[xlabel], merged_df[ylabel], color = merged_df.color ) \n",
    "plt.xlabel(xlabel+ ' (mg/mL)')\n",
    "plt.ylabel(ylabel);\n",
    "\n",
    "\n",
    "if 'new_handles' in dir():\n",
    "    for ax in [ax1, ax2]:\n",
    "        # Set the modified handles and labels in the legend\n",
    "        legend = ax.legend(handles=new_handles, labels=new_labels, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "        # Set the colors of legend lines based on label_colors dictionary\n",
    "        for line, label in zip(legend.get_lines(), new_labels):\n",
    "            line.set_color(label_colors[label])\n",
    "\n",
    "        # Set the alpha value of legend lines to 1.0 (no transparency)\n",
    "        for line in legend.get_lines():\n",
    "            line.set_alpha(1.0)\n",
    "\n",
    "### previous output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626fc172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df= combined_df.drop([13])\n",
    "\n",
    "fig0, ax0 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'PEG 20000 concentration'\n",
    "ylabel = 'power n'\n",
    "plt.scatter(combined_df[xlabel], combined_df[ylabel], color = 'red' , label = \"R&A's spreadsheet\")\n",
    "ylabel = 'alpha'\n",
    "plt.plot(combined_df[xlabel], combined_df[ylabel], '.',color = 'blue', label = \"Viva's analysis\" )\n",
    "plt.xlabel(xlabel+ ' (mg/mL)')\n",
    "plt.ylabel('eMSD Exponent');\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0, ax0 = plt.subplots(1, 1, figsize = [3,3])\n",
    "xlabel = 'PEG 20000 concentration'\n",
    "ylabel = 'power n'\n",
    "plt.scatter(combined_df[xlabel], combined_df[ylabel], color = 'red' , label = \"R&A's spreadsheet\")\n",
    "ylabel = 'alpha'\n",
    "plt.plot(combined_df[xlabel], combined_df[ylabel], '.',color = 'blue', label = \"Viva's analysis\" )\n",
    "plt.xlabel(xlabel+ ' (mg/mL)')\n",
    "plt.ylabel('eMSD Exponent');\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c06c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['control_tracer.pkl','power n', 'alpha','excess kurtosis (from cell 47) (frame 15)']] # There's no difference between the exponent Bess obtained and the exponent I obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0aa01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emsd_list) == len(shortfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2fb2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot derivative of MSD plot.\n",
    "\n",
    "plt.axhline(1, lw = 1 ,color = 'k')\n",
    "for this_emsd, color, description in zip(emsd_list, combined_df.color, combined_df.Description):\n",
    "\n",
    "    #plt.plot(np.log10(this_emsd.index),np.log10(this_emsd), '.')\n",
    "\n",
    "    # Calculate the discrete derivative\n",
    "    x = np.log10(this_emsd.index)\n",
    "    y = np.log10(this_emsd)\n",
    "    dy_dx = np.diff(y) / np.diff(x)\n",
    "\n",
    "    # Plot the derivative\n",
    "    plt.plot(x[1:], dy_dx, '.', color = color, lw = 0.8)\n",
    "    plt.plot(x[1:], dy_dx, color = color, lw = 0.8, alpha = 0.3)\n",
    "    \n",
    "    # Apply smoothing to the derivative\n",
    "    smoothed_dy_dx = savgol_filter(dy_dx, window_length=9, polyorder=3)\n",
    "    plt.plot(x[1:], smoothed_dy_dx, label=description, color=color, lw = 1) # Plot the smoothed derivative\n",
    "\n",
    "    plt.xlabel('log10 (time (s))')\n",
    "    plt.ylabel('d log10 eMSD /d logt')\n",
    "\n",
    "#plt.xlim(xmax = 0.25)\n",
    "#plt.ylim(ymin = 0, ymax = 1.8)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8025e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['control_tracer.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot smoothed derivative\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "plt.axhline(1, lw=0.5, color='gray')\n",
    "\n",
    "palette_name = \"tab20b\" # Set the color palette\n",
    "palette = sns.color_palette(palette_name) # Get the color palette\n",
    "\n",
    "for i, (this_emsd, data_file) in enumerate(zip(emsd_list, combined_df['movie'])):\n",
    "    x = np.log10(this_emsd.index)\n",
    "    y = np.log10(this_emsd)\n",
    "    dy_dx = np.diff(y) / np.diff(x)\n",
    "\n",
    "    # Apply smoothing to the derivative\n",
    "    smoothed_dy_dx = savgol_filter(dy_dx, window_length=30, polyorder=3)\n",
    "\n",
    "    # Plot the smoothed derivative with the corresponding color from the palette\n",
    "    plt.plot(x[1:], smoothed_dy_dx, label=data_file, color=palette[i % len(palette)], alpha=1, lw=3)\n",
    "\n",
    "\n",
    "plt.xlabel('log10 (time (s))')\n",
    "plt.ylabel('d/dt log10 (eMSD)')\n",
    "plt.xlim(xmax = 0)\n",
    "plt.ylim(ymin=0.6, ymax=1.2)\n",
    "plt.title('smoothed')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3693cf",
   "metadata": {},
   "source": [
    "# Van Hove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83857aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kurtosis_df_and_van_hove(file, filepath, df, fps, this_description, maxlagtime, skip, scaling, binsequence):\n",
    "\n",
    "        tracksbyframex = df.set_index(['frame', 'particle'])['x'].unstack()\n",
    "        tracksbyframey = df.set_index(['frame', 'particle'])['y'].unstack()\n",
    "        trackshistlistx = []\n",
    "        trackshistlisty = []\n",
    "        dataframes = []\n",
    "        this_lagtimelist = []\n",
    "        warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "        j = 0\n",
    "        for vanhove_lagtime_inframes in range(1, maxlagtime + 1, skip):  \n",
    "            this_time = vanhove_lagtime_inframes / fps\n",
    "            this_lagtimelist.append(this_time)\n",
    "\n",
    "            # calculate vanhove / probability distribution function\n",
    "            trackshistx = manualnorm(tp.motion.vanhove(\n",
    "                tracksbyframex, lagtime=vanhove_lagtime_inframes, mpp=scaling, bins=binsequence, ensemble=True))\n",
    "            trackshistx.name = 'x Probability distribution'\n",
    "            trackshisty = manualnorm(tp.motion.vanhove(\n",
    "                tracksbyframey, lagtime=vanhove_lagtime_inframes, mpp=scaling, bins=binsequence, ensemble=True))\n",
    "            trackshisty.name = 'y Probability distribution'\n",
    "\n",
    "            trackshistlistx.append(trackshistx)\n",
    "            trackshistlisty.append(trackshisty)\n",
    "            \n",
    "            kx = trackshistx.kurtosis()\n",
    "            ky = trackshisty.kurtosis()\n",
    "\n",
    "            dict_row_pair = {'File': [filepath]*2,\n",
    "                'Description': [this_description]*2,\n",
    "                'time (s)': [this_time]*2,\n",
    "                'kurtosis': [kx, ky],\n",
    "                'xy': ['x', 'y'],\n",
    "                'histogram': [trackshistx, trackshisty],          \n",
    "                }\n",
    "            \n",
    "            dataframes.append(pd.DataFrame(dict_row_pair))\n",
    "            j += 1\n",
    "        \n",
    "        warnings.filterwarnings(\"once\", category=RuntimeWarning)\n",
    "        \n",
    "        one_movie_kurtosis_df = pd.concat(dataframes)\n",
    "        return one_movie_kurtosis_df, trackshistlistx, trackshistlisty, this_lagtimelist\n",
    "\n",
    "def create_sigma_df(this_description, filepath, this_lagtimelist, trackshistlistx, trackshistlisty):\n",
    "    dataframes = []\n",
    "    gaussian_fit_paramsxlist = []\n",
    "    gaussian_fit_paramsylist = []\n",
    "    gaussian_fit_covmxlist = []\n",
    "    gaussian_fit_covmylist = []\n",
    "\n",
    "    #j = 0\n",
    "    for vanhove_lagtime_insecs, trackshist, trackshisty in zip(\n",
    "        this_lagtimelist, trackshistlistx, trackshistlisty\n",
    "    ):\n",
    "\n",
    "        gaussian_fit_paramsx, gaussian_fit_covmx = sco.curve_fit(gaussian, trackshist.index, trackshist.values)\n",
    "        gaussian_fit_paramsy, gaussian_fit_covmy = sco.curve_fit(gaussian, trackshisty.index, trackshisty.values)\n",
    "\n",
    "        sigma_x_stderr = np.sqrt(gaussian_fit_covmx[sigma_index, sigma_index])\n",
    "        sigma_y_stderr = np.sqrt(gaussian_fit_covmy[sigma_index, sigma_index])\n",
    "\n",
    "        dict_row_pair = {'File': [filepath]*2,\n",
    "                'Description': [this_description]*2,\n",
    "                'time (s)': [vanhove_lagtime_insecs]*2,\n",
    "                         ## Gaussian width should be positive:\n",
    "                'width sigma': [abs(gaussian_fit_paramsx[sigma_index]), abs(gaussian_fit_paramsy[sigma_index])],\n",
    "                'sigma stderr': [sigma_x_stderr, sigma_y_stderr],\n",
    "                'xy': ['x', 'y'],\n",
    "                'gaussian_param_height': [gaussian_fit_paramsx[height_index], gaussian_fit_paramsy[height_index]],\n",
    "                'gaussian_param_center': [gaussian_fit_paramsx[center_index], gaussian_fit_paramsy[center_index]],\n",
    "                'gaussian_fit_cov':[gaussian_fit_covmx, gaussian_fit_covmy],\n",
    "                        }\n",
    "        dataframes.append(pd.DataFrame(dict_row_pair))\n",
    "        \n",
    "        ## More info that might be useful to save as lists.\n",
    "        gaussian_fit_paramsxlist.append(gaussian_fit_paramsx)\n",
    "        gaussian_fit_paramsylist.append(gaussian_fit_paramsy) # gaussian width might need abs still\n",
    "        gaussian_fit_covmxlist.append(gaussian_fit_covmx)\n",
    "        gaussian_fit_covmylist.append(gaussian_fit_covmy)\n",
    "\n",
    "        #j += 1\n",
    "    \n",
    "    one_movie_sigma_df = pd.concat(dataframes)\n",
    "    \n",
    "    return one_movie_sigma_df, \\\n",
    "        gaussian_fit_paramsxlist, gaussian_fit_paramsylist, gaussian_fit_covmxlist, gaussian_fit_covmylist\n",
    "\n",
    "def remove_dup_columns(df):\n",
    "    \n",
    "    # Get a boolean mask of duplicated columns\n",
    "    duplicate_mask = df.columns.duplicated()\n",
    "    # Get the column indexes to keep (excluding duplicates)\n",
    "    column_indexes_to_keep = [i for i in range(len(df.columns)) if not duplicate_mask[i]]\n",
    "    # Create a new DataFrame with the selected columns (excluding duplicates)\n",
    "    new_df = df.iloc[:, column_indexes_to_keep]\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_van_hove_slow = False\n",
    "# It takes about two minutes to do 5 lagtimes.\n",
    "# 120 lagtimes takes 73 minutes or 117 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate probability distribution function (van hove)\n",
    "\n",
    "if calculate_van_hove_slow:\n",
    "    maxlagtime = 120#40 # in number of frames  \n",
    "            # for 120, maybe vanhove_max of 7. for 40, perhaps 4.  (120 took 99 minutes.)\n",
    "    skip = 1#int(maxlagtime/3)\n",
    "    vanhove_max_x = 7\n",
    "    binwidth = 0.04\n",
    "    figsize = [8, 8]\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    #warnings.filterwarnings(\"once\", category=DeprecationWarning)\n",
    "\n",
    "    # initializations before for loop\n",
    "    binsequence = np.arange(-vanhove_max_x, vanhove_max_x, binwidth)\n",
    "    trackshistlistlist = []\n",
    "    trackshistlistlisty = []\n",
    "    kurtosisdflist = []\n",
    "    #kurtosisdflisty = []\n",
    "    color_mapping = {}\n",
    "    numplots = int(maxlagtime / skip)\n",
    "    i = 0\n",
    "    \n",
    "    gaussian_df_list = []\n",
    "    gaussian_fit_paramsxlistlist = []\n",
    "    gaussian_fit_paramsylistlist = []\n",
    "    gaussian_fit_covmxlistlist = []\n",
    "    gaussian_fit_covmylistlist = []\n",
    "    lagtimelistlist = []\n",
    "    colorlist = []\n",
    "        \n",
    "    ## Calculate the Gaussian fitting parameters and store the information\n",
    "    do_curvefit = True\n",
    "\n",
    "\n",
    "\n",
    "    assert(len(shortfiles) ==len(df_list_nodrift)) \n",
    "    \n",
    "    \n",
    "\n",
    "    ## Iterate through each movie\n",
    "    for (row_tuple, file, filepath, df, fps, ii) in tqdm_notebook(zip(combined_df.itertuples(), \n",
    "                                                             combined_df['control_tracer.pkl'], \n",
    "                                                             combined_df['Analysis file'], \n",
    "                                                             df_list_nodrift, \n",
    "                                                             combined_df.fps, \n",
    "                                                             range(len(combined_df))),\n",
    "                                                                 total = len(df_list_nodrift)):\n",
    "        if AlexRebecca:\n",
    "            this_description = re.sub(r'\\(\\d\\)$', '', file).strip()\n",
    "        else:\n",
    "            this_description = row_tuple.Description\n",
    "\n",
    "        try:\n",
    "            color = row_tuple.color # maybe combined_df already has a color column\n",
    "        except:\n",
    "            if this_description not in color_mapping:\n",
    "                color_mapping[this_description] = plt.rcParams['axes.prop_cycle'].by_key()['color'][i % len(plt.rcParams['axes.prop_cycle'])]\n",
    "                i += 1\n",
    "\n",
    "            color = color_mapping[this_description]\n",
    "\n",
    "        ## Calculate van hove and kurtosis for a range of lagtimes\n",
    "        one_movie_kurtosis_df, trackshistlist, trackshistlisty, this_lagtimelist = \\\n",
    "            create_kurtosis_df_and_van_hove(file, filepath, df, fps, this_description, \n",
    "                               maxlagtime, skip, scaling, binsequence)\n",
    "        \n",
    "        kurtosisdflist.append(one_movie_kurtosis_df)\n",
    "        trackshistlistlist.append(trackshistlist)\n",
    "        trackshistlistlisty.append(trackshistlisty)\n",
    "        colorlist.append(color)\n",
    "\n",
    "\n",
    "        if do_curvefit:\n",
    "            one_movie_sigma_df, \\\n",
    "                gaussian_fit_paramsxlist, gaussian_fit_paramsylist, gaussian_fit_covmxlist, gaussian_fit_covmylist = \\\n",
    "                create_sigma_df(this_description, filepath, this_lagtimelist, trackshistlist, trackshistlisty)\n",
    "\n",
    "            \"\"\"\n",
    "            #== cut\n",
    "            gaussian_fit_paramsxlist = []\n",
    "            gaussian_fit_paramsylist = []\n",
    "            gaussian_fit_covmxlist = []\n",
    "            gaussian_fit_covmylist = []\n",
    "\n",
    "            j = 0\n",
    "            for vanhove_lagtime_insecs, trackshist, trackshisty in zip(\n",
    "                this_lagtimelist, trackshistlist, trackshistlisty\n",
    "            ):\n",
    "\n",
    "                gaussian_fit_paramsx, gaussian_fit_covmx = sco.curve_fit(gaussian, trackshist.index, trackshist.values)\n",
    "                gaussian_fit_paramsy, gaussian_fit_covmy = sco.curve_fit(gaussian, trackshisty.index, trackshisty.values)\n",
    "\n",
    "                sigma_x_stderr = np.sqrt(gaussian_fit_covmx[sigma_index, sigma_index])\n",
    "                sigma_y_stderr = np.sqrt(gaussian_fit_covmy[sigma_index, sigma_index])\n",
    "\n",
    "                \n",
    "                for p in range(2):\n",
    "                    big_filepathlist.append(filename)\n",
    "                    big_descriptionlist.append(remove_number_parentheses(file))\n",
    "                    big_tlist.append(vanhove_lagtime_insecs)\n",
    "\n",
    "                big_xy_list.append('x')\n",
    "                big_sigmalist.append(gaussian_fit_paramsx[sigma_index])\n",
    "                big_sigma_stderr.append(sigma_x_stderr)\n",
    "                \n",
    "                big_xy_list.append('y')\n",
    "                big_sigmalist.append(gaussian_fit_paramsy[sigma_index])\n",
    "                big_sigma_stderr.append(sigma_y_stderr)\n",
    "\n",
    "                gaussian_fit_paramsxlist.append(gaussian_fit_paramsx)\n",
    "                gaussian_fit_paramsylist.append(gaussian_fit_paramsy)\n",
    "                gaussian_fit_covmxlist.append(gaussian_fit_covmx)\n",
    "                gaussian_fit_covmylist.append(gaussian_fit_covmy)\n",
    "\n",
    "                j += 1\n",
    "                #==cut\n",
    "            \"\"\"\n",
    "                \n",
    "\n",
    "            # Store the lists for each file and lag time\n",
    "            gaussian_df_list.append(one_movie_sigma_df)\n",
    "            lagtimelistlist.append(this_lagtimelist)\n",
    "            gaussian_fit_paramsxlistlist.append(gaussian_fit_paramsxlist)\n",
    "            gaussian_fit_paramsylistlist.append(gaussian_fit_paramsylist)\n",
    "            gaussian_fit_covmxlistlist.append(gaussian_fit_covmxlist)\n",
    "            gaussian_fit_covmylistlist.append(gaussian_fit_covmylist)\n",
    "    \n",
    "\n",
    "    \n",
    "    big_kurtosis_df = pd.concat(kurtosisdflist)\n",
    "    big_kurtosis_df = big_kurtosis_df.set_index([\"time (s)\", \"File\", \"xy\"])\n",
    "\n",
    "    big_gaussian_df = pd.concat(gaussian_df_list)\n",
    "    big_gaussian_df = big_gaussian_df.set_index([\"time (s)\", \"File\", \"xy\"])\n",
    "\n",
    "    \n",
    "    big_data_df = pd.concat([big_kurtosis_df, big_gaussian_df], axis = 1)\n",
    "    \n",
    "    ## Saving results\n",
    "    # Create a dictionary to save\n",
    "    data_dict_vanhove = {\n",
    "        'filename_list': filename_list,\n",
    "        'binsequence': binsequence,\n",
    "        'trackshistlistlist': trackshistlistlist,\n",
    "        'trackshistlistlisty': trackshistlistlisty,\n",
    "        'kurtosisdflist': kurtosisdflist,\n",
    "        #'kurtosisdflisty': kurtosisdflisty,\n",
    "        'gaussian_fit_paramsxlistlist': gaussian_fit_paramsxlistlist,\n",
    "        'gaussian_fit_paramsylistlist': gaussian_fit_paramsylistlist,\n",
    "        'gaussian_fit_covmxlistlist': gaussian_fit_covmxlistlist,\n",
    "        'gaussian_fit_covmylistlist': gaussian_fit_covmylistlist,\n",
    "        'lagtimelistlist': lagtimelistlist,\n",
    "        'maxlagtime': maxlagtime,\n",
    "        'skip': skip,\n",
    "        'scaling': scaling,\n",
    "        'fps': fps,\n",
    "        'colorlist': colorlist,\n",
    "        'big_data_df': big_data_df, # added 2023-06-22\n",
    "    }\n",
    "\n",
    "    today = date.today().isoformat() # Get today's date in ISO format\n",
    "    pickle_file = f'data_dict_vanhove_{today}_{len(this_lagtimelist)}lagtimes.pkl' # Create the pickle file name\n",
    "    \n",
    "    os.chdir(savefolder)\n",
    "    # Save the dictionary to a pickle file\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(data_dict_vanhove, f)\n",
    "        \n",
    "    print(\"Saved:\", os.path.join(savefolder, pickle_file))   \n",
    "    beep()\n",
    "\n",
    "    #calculate_van_hove_slow = False\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(\"Execution time:\", execution_time_minutes, \"minutes\")\n",
    "else: # open saved file\n",
    "    root = tk.Tk() # Create a Tkinter root window\n",
    "    root.lift()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    \n",
    "    print('Look for the file dialogue window!')\n",
    "    beep()\n",
    "    beep()\n",
    "\n",
    "    # Open the file dialog to select the pickle file\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Pickle Files\", \"*.pkl\")])\n",
    "\n",
    "    # Check if a file was selected\n",
    "    if file_path:\n",
    "        # Process the selected pickle file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data_dict_vanhove = pickle.load(f)\n",
    "        # Extract the variables from the dictionary\n",
    "        filename_list = data_dict_vanhove['filename_list']\n",
    "        binsequence = data_dict_vanhove['binsequence']\n",
    "        trackshistlistlist = data_dict_vanhove['trackshistlistlist']\n",
    "        trackshistlistlisty = data_dict_vanhove['trackshistlistlisty']\n",
    "        kurtosisdflist = data_dict_vanhove['kurtosisdflist']\n",
    "        try:\n",
    "            kurtosisdflisty = data_dict_vanhove['kurtosisdflisty']\n",
    "            print('Using kurtosisdflisty')\n",
    "        except:\n",
    "            pass\n",
    "        gaussian_fit_paramsxlistlist = data_dict_vanhove['gaussian_fit_paramsxlistlist']\n",
    "        gaussian_fit_paramsylistlist = data_dict_vanhove['gaussian_fit_paramsylistlist']\n",
    "        gaussian_fit_covmxlistlist = data_dict_vanhove['gaussian_fit_covmxlistlist']\n",
    "        gaussian_fit_covmylistlist = data_dict_vanhove['gaussian_fit_covmylistlist']\n",
    "        try:\n",
    "            lagtimelist = data_dict_vanhove['lagtimelist']  # in seconds\n",
    "        except:\n",
    "            lagtimelistlist = data_dict_vanhove['lagtimelistlist']  # in seconds\n",
    "        maxlagtime = data_dict_vanhove['maxlagtime']  # in number of frames\n",
    "        skip = data_dict_vanhove['skip']  # in number of frames\n",
    "        scaling = data_dict_vanhove['scaling']  # in microns per pixel\n",
    "        fps = data_dict_vanhove['fps']  # in Hz\n",
    "        try:\n",
    "            big_data_df = data_dict_vanhove['big_data_df']\n",
    "            \n",
    "            ## If big_data_df has duplicated columns, this will remove them.\n",
    "            # Get a boolean mask of duplicated columns\n",
    "            duplicate_mask = big_data_df.columns.duplicated()\n",
    "            # Get the column indexes to keep (excluding duplicates)\n",
    "            column_indexes_to_keep = [i for i in range(len(big_data_df.columns)) if not duplicate_mask[i]]\n",
    "            # Create a new DataFrame with the selected columns\n",
    "            big_data_df = big_data_df.iloc[:, column_indexes_to_keep]\n",
    "        except:\n",
    "            print('Could not extract big_data_df; this must be an older file.')\n",
    "        try:\n",
    "            colorlist = data_dict_vanhove['colorlist']\n",
    "        except:\n",
    "            print('Could not extract colorlist; this is a file from June 29 or earlier.')\n",
    "\n",
    "        print('Loaded file', file_path)\n",
    "    else:\n",
    "        print(\"No file selected.\")\n",
    "        \n",
    "try:\n",
    "    big_data_df = big_data_df.reset_index()\n",
    "    big_data_df = remove_dup_columns(big_data_df)\n",
    "    \n",
    "    big_data_df['width sigma'] = abs(big_data_df['width sigma']) # width should be positive\n",
    "    \n",
    "    ## Add color column to big_data_df.\n",
    "    big_data_df = big_data_df.merge(combined_df[['Analysis file', 'color']], left_on='File', right_on='Analysis file', how='left')\n",
    "\n",
    "except NameError:\n",
    "    print('No big_data_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02273eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f24661",
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosisdf = kurtosisdflist[0]\n",
    "kurtosisdf.File.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac8db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Analysis file'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(big_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a32be",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlagtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49084e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'lagtimelist' in dir():\n",
    "        assert  (int(maxlagtime/skip) == len(lagtimelist))\n",
    "except AssertionError:\n",
    "    if 'this_lagtimelist' in dir():\n",
    "        assert ((int(maxlagtime/skip) == len(this_lagtimelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assert(len(big_gaussian_df) == len(shortfiles) * len(this_lagtimelist) * 2) # 2 for xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b970cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.scatter(1/(big_data_df['width sigma']), big_data_df['kurtosis'], color =big_data_df.color, alpha = .15 )\n",
    "    plt.ylabel('kurtosis')\n",
    "    plt.xlabel('1/Gaussian Sigma (1/$\\mu$m)')\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.xscale(\"log\")\n",
    "except:\n",
    "    plt.scatter(1/(big_data_df['width sigma']), big_data_df['kurtosis']) #, color =big_data_df.color, alpha = .15 )\n",
    "    plt.ylabel('kurtosis')\n",
    "    plt.xlabel('1/Gaussian Sigma (1/$\\mu$m)')\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.xscale(\"log\")\n",
    "\n",
    "\n",
    "if saving:\n",
    "    datestr = datestring()\n",
    "    savefigure(datestr + 'kurtosis vs gaussian width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9b006",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df['time (s)'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df['time (s)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c316606",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(big_data_df['time (s)'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ecd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data_df['time (s)'].unique()[[5,10, -20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a664d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saving = True\n",
    "\n",
    "## key, plot title, plot ylabel\n",
    "plotable1 = ('width sigma', 'Gaussian Sigma', 'Gaussian Sigma ($\\mu$m)' )\n",
    "plotable2 = ('kurtosis', 'kurtosis', 'kurtosis')\n",
    "\n",
    "\n",
    "for time_value in big_data_df['time (s)'].unique()[[5,10, -20,-1]]:\n",
    "#for time_value in [0.05784025, 0.40488172,0.40976889, 3.065533,2.9913129, 3.07326668 ]:#np.sort(big_data_df['time (s)'].unique()):\n",
    "#for time_value in np.sort(big_data_df['time (s)'].unique()):\n",
    "    for plotable in [plotable1, plotable2]:\n",
    "        key = plotable[0]\n",
    "        plottitle = plotable[1]\n",
    "        ylabel = plotable[2]\n",
    "    \n",
    "        # Filter the DataFrame based on approximate equality\n",
    "        data = big_data_df[np.isclose(big_data_df['time (s)'], time_value)]\n",
    "\n",
    "        # Define the order for the horizontal axis (largest to smallest)\n",
    "        order = data.groupby('Description')[key].mean().sort_values(ascending=False).index\n",
    "\n",
    "        # Get unique color values from the 'color' column\n",
    "        color_palette = list(data['color'].unique())\n",
    "\n",
    "        if debug:\n",
    "            display(data[['Description', 'color']])\n",
    "            display(color_palette)\n",
    "            display(order)\n",
    "\n",
    "        # Create a dictionary to map description to color\n",
    "        color_dict = dict(zip(data['Description'], data['color']))\n",
    "\n",
    "        # Generate color_palette in the desired order\n",
    "        color_palette = [color_dict[desc] for desc in order if desc in color_dict]\n",
    "\n",
    "        # Create a figure and axes\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot the violin plot with the custom color palette\n",
    "        sns.violinplot(x='Description', y=key, data=data, \n",
    "                       #color='gray', \n",
    "                       palette=color_palette,\n",
    "                       #zorder=1,\n",
    "                       inner = None, # no automatic boxplot\n",
    "                       split=False, order=order)\n",
    "        for violin in ax.collections:\n",
    "            violin.set_alpha(0.5)\n",
    "            violin.set_zorder(1)  # Set higher zorder for the violins\n",
    "\n",
    "\n",
    "        sns.boxplot(x='Description', y=key, data=data, order = order,palette=color_palette,\n",
    "                    showfliers=False, # don't show outliers\n",
    "                   width=0.06, #zorder=2,\n",
    "                )\n",
    "\n",
    "        #sns.stripplot(x='Description', y=key, hue='xy', data=data, order=order, #palette='dark', \n",
    "        #              dodge=True,  jitter=0.01, size=5)\n",
    "        sns.swarmplot(x='Description', y=key, hue='xy', data=data, order=order, palette='dark', \n",
    "                  dodge=True, size=5, edgecolor='gray', linewidth=0.5, )\n",
    "\n",
    "\n",
    "        plt.xlabel('')\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(plottitle + ' at Time ' + str(time_value) + ' s')\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        #plt.legend(title='Variable')\n",
    "\n",
    "        os.chdir(r'C:\\Users\\vhorowit\\Documents\\fig-expt')\n",
    "        datestr = datestring()\n",
    "        if saving:\n",
    "            savefigure(datestr + ' ' +  plottitle + ' violin')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filename_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ea306",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kurtosisdflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2dc734",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot previously calculated probability distribution function\n",
    "\n",
    "showprobdist = False\n",
    "show_curvefit = True\n",
    "plots_per_row = 5\n",
    "debug = False\n",
    "\n",
    "\n",
    "if showprobdist:\n",
    "    try:\n",
    "        numplots =  big_data_df['time (s)'].nunique()\n",
    "    except:\n",
    "        numplots = int(maxlagtime / skip)\n",
    "    plots_per_column = int(math.ceil(numplots / plots_per_row))\n",
    "    figsize = [8, max(2, plots_per_column*1.5)]\n",
    "    fig, axs = plt.subplots(plots_per_column, plots_per_row, figsize=figsize, dpi=300)\n",
    "    plt.subplots_adjust(top=.96)\n",
    "    fig3, axsy = plt.subplots(plots_per_column, plots_per_row, figsize=figsize, dpi=300)\n",
    "    plt.subplots_adjust(top=.96)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.4)  # Adjust the spacing between subplots\n",
    "\n",
    "    for i in range(plots_per_column-1):\n",
    "        for j in range(plots_per_row):\n",
    "            axs[i, j].set_xticklabels([])  # Set shared x-axis for axs\n",
    "            axs[i, j].tick_params(bottom=True, labelbottom=True)  # Customize tick labels for the last row of axs\n",
    "\n",
    "            axsy[i, j].set_xticklabels([])  # Set shared x-axis for axsy\n",
    "            axsy[i, j].tick_params(bottom=True, labelbottom=True)  # Customize tick labels for the last row of axsy\n",
    "\n",
    "    fig.suptitle('Probability distribution in x direction')\n",
    "    fig3.suptitle('Probability distribution in y direction')\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, )\n",
    "fig4, ax4 = plt.subplots(1, 1, )\n",
    "\n",
    "color_mapping = {}\n",
    "if showprobdist:\n",
    "    if not show_curvefit:\n",
    "        xlimx = np.zeros_like(axs.flat)\n",
    "        xlimy = np.zeros_like(axs.flat)\n",
    "        ylimx = np.zeros_like(axsy.flat)\n",
    "        ylimy = np.zeros_like(axsy.flat)\n",
    "    \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) # Not my fault. There's some dependency.\n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "if 'big_data_df' in dir():\n",
    "    \n",
    "    # Sort the DataFrame by 'time (s)'\n",
    "    big_data_df_time_sorted = big_data_df.sort_values('time (s)')\n",
    "    prevtime = big_data_df_time_sorted['time (s)'].iloc[0]\n",
    "    \n",
    "    for index, row in tqdm_notebook(big_data_df_time_sorted.iterrows(), total = len(big_data_df_time_sorted)):\n",
    "        color = row.color #color_mapping[row.Description]\n",
    "        \n",
    "        if not np.isclose(row['time (s)'], prevtime):\n",
    "            if debug:\n",
    "                print('New time:',1000*row['time (s)'], 'ms. Index:', index, '. Previous time:', prevtime*1000, 'ms' )\n",
    "            j += 1\n",
    "        prevtime = row['time (s)']\n",
    "\n",
    "        if showprobdist:\n",
    "            if row.xy == 'x':\n",
    "                ax = axs.flat[j]\n",
    "            else:\n",
    "                ax = axsy.flat[j]\n",
    "\n",
    "            if show_curvefit:\n",
    "                try:\n",
    "                    #display(row.histogram.index)\n",
    "                    gaussian_fit = gaussian(row.histogram.index, row.gaussian_param_height,\n",
    "                                             row.gaussian_param_center, row['width sigma'])\n",
    "\n",
    "                    ax.plot(row.histogram.index, gaussian_fit, color=color, linewidth=0.5, alpha=0.3)\n",
    "\n",
    "                    if 'xlimx' in dir() and xlimx[j] != 0:\n",
    "                        if xlimx[j][0] < 0: ## error checking. I don't know why this would be negative but.\n",
    "                            (axs.flat[j]).set_xlim(xmax = xlimx[j][1])\n",
    "                        else:\n",
    "                            (axs.flat[j]).set_xlim(xlimx[j])\n",
    "                        if ylimx[j][0] < 0:\n",
    "                            (axs.flat[j]).set_ylim(ymax = ylimx[j][1])\n",
    "                        else:\n",
    "                            (axs.flat[j]).set_ylim(ylimx[j])\n",
    "                        if xlimy[j][0] < 0:\n",
    "                            (axs.flat[j]).set_xlim(xmax = xlimy[j][1])\n",
    "                        else:\n",
    "                            (axsy.flat[j]).set_xlim(xlimy[j])\n",
    "                        if ylimy[j][0] < 0:\n",
    "                            (axs.flat[j]).set_ylim(ymax = ylimy[j][1])\n",
    "                        else:\n",
    "                            (axsy.flat[j]).set_ylim(ylimy[j])\n",
    "                except:\n",
    "                    print('Failed to show Gaussian fits.')\n",
    "            #ax4.scatter(x=vanhove_lagtime_insecs,y=gaussian_fit_paramsx[2], color = color, alpha = 0.3)\n",
    "\n",
    "            # Plot vanhove datapoints \n",
    "            row.histogram.plot(\n",
    "                marker=\".\",\n",
    "                markersize=1,\n",
    "                linestyle=\"\",\n",
    "                ax=ax,\n",
    "                label=file,\n",
    "                color=color,\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "            ax.set_title(\"{:.2f} ms\".format(1000 * row['time (s)']))\n",
    "            ax.set_xticks([int(row.histogram.index.min())+1, 0, int(row.histogram.index.max())])\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "            if not show_curvefit:\n",
    "                xlimx[j] = axs.flat[j].get_xlim()\n",
    "                ylimx[j] = axs.flat[j].get_ylim()\n",
    "                xlimy[j] = axsy.flat[j].get_xlim()\n",
    "                ylimy[j] = axsy.flat[j].get_ylim()\n",
    "\n",
    "            \n",
    "    ## Plot gaussian widths versus lag time\n",
    "    for lagtimelist, gaussian_fit_paramsxlist, gaussian_fit_paramsylist, color in \\\n",
    "        zip(lagtimelistlist,gaussian_fit_paramsxlistlist,gaussian_fit_paramsylistlist, colorlist):\n",
    "        # Gaussian widths\n",
    "        ax4.plot(lagtimelist, np.abs(np.array(gaussian_fit_paramsxlist)[:, 2]), alpha = .5, color = color)\n",
    "        ax4.plot(lagtimelist, np.abs(np.array(gaussian_fit_paramsylist)[:, 2]),  alpha = .5, color = color)\n",
    "\n",
    "    ## Plot kurtosis versus lag time\n",
    "    # Create empty lists to store handles and labels\n",
    "    handles = []\n",
    "    labels = []\n",
    "\n",
    "    # Dictionary to map labels to colors\n",
    "    label_colors = {}\n",
    "\n",
    "    for lagtimelist, kurtosisdf, color in zip(lagtimelistlist, kurtosisdflist, colorlist):\n",
    "        for dim in ['x', 'y']:\n",
    "            index = combined_df[combined_df['Analysis file'] == kurtosisdf.File.iloc[0]].index[0]\n",
    "            color = combined_df.loc[index, 'color']\n",
    "            label = combined_df.loc[index, 'Description']\n",
    "\n",
    "            # Plot Kurtosis\n",
    "            kurtosisdfslice = kurtosisdf[kurtosisdf.xy == dim]\n",
    "            line = kurtosisdfslice.set_index(\"time (s)\").plot(ax=ax2, color=color, alpha=0.3, label=f'{label} {dim} kurtosis')\n",
    "\n",
    "            # Get the handle and label of the plotted line\n",
    "            line_handle, _ = line.get_legend_handles_labels()\n",
    "\n",
    "            # Append the handle and label to the lists\n",
    "            handles.extend(line_handle)\n",
    "            labels.append(label)\n",
    "\n",
    "            # Map label to color\n",
    "            if label not in label_colors:\n",
    "                label_colors[label] = color\n",
    "\n",
    "    # Consolidate the figure legend to just the unique labels\n",
    "    new_handles = []\n",
    "    new_labels = []\n",
    "    handle_mapping = {}\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label not in handle_mapping:\n",
    "            handle_mapping[label] = handle\n",
    "            new_handles.append(handle)\n",
    "            new_labels.append(label)\n",
    "\n",
    "    for ax in [ax2, ax4]:\n",
    "        # Set the modified handles and labels in the legend\n",
    "        legend = ax.legend(handles=new_handles, labels=new_labels, bbox_to_anchor=(1, 1), loc='upper left')\n",
    "\n",
    "        # Set the colors of legend lines based on label_colors dictionary\n",
    "        for line, label in zip(legend.get_lines(), new_labels):\n",
    "            line.set_color(label_colors[label])\n",
    "\n",
    "        # Set the alpha value of legend lines to 1.0 (no transparency)\n",
    "        for line in legend.get_lines():\n",
    "            line.set_alpha(1.0)\n",
    "\n",
    "    \n",
    "\n",
    "elif 'kurtosisdflisty' in dir():\n",
    "\n",
    "    assert len(filename_list) == len(trackshistlistlist)\n",
    "    assert len(filename_list) == len(trackshistlistlisty)\n",
    "    assert len(filename_list) == len(kurtosisdflist)\n",
    "    assert len(filename_list) == len(kurtosisdflisty)\n",
    "\n",
    "\n",
    "\n",
    "    for file, lagtimelist, trackshistlist, trackshistlisty, \\\n",
    "        kurtosisdf, kurtosisdfy, gaussian_fit_paramsxlist, gaussian_fit_paramsylist \\\n",
    "        in tqdm_notebook(\n",
    "            zip(shortfiles,lagtimelistlist, trackshistlistlist, trackshistlistlisty, \\\n",
    "                kurtosisdflist, kurtosisdflisty, gaussian_fit_paramsxlistlist, gaussian_fit_paramsylistlist),\n",
    "        total=len(shortfiles),\n",
    "    ):\n",
    "        # Vary colors if base_filename is different\n",
    "        base_filename = re.sub(r\"\\(\\d\\)$\", \"\", file).strip()\n",
    "        if base_filename not in color_mapping:\n",
    "            color_mapping[base_filename] = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][\n",
    "                i % len(plt.rcParams[\"axes.prop_cycle\"])\n",
    "            ]\n",
    "            i += 1\n",
    "        color = color_mapping[base_filename]\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        for vanhove_lagtime_insecs, trackshist, trackshisty, gaussian_fit_paramsx, gaussian_fit_paramsy in zip(\n",
    "            lagtimelist, trackshistlist, trackshistlisty, gaussian_fit_paramsxlist, gaussian_fit_paramsylist\n",
    "        ):\n",
    "            if showprobdist:\n",
    "                if show_curvefit:\n",
    "                    try:\n",
    "                        gaussian_fitx = gaussian(trackshist.index, gaussian_fit_paramsx[0],\n",
    "                                                 gaussian_fit_paramsx[1], gaussian_fit_paramsx[2])\n",
    "                        gaussian_fity = gaussian(trackshisty.index, gaussian_fit_paramsy[0],\n",
    "                                                 gaussian_fit_paramsy[1], gaussian_fit_paramsy[2])\n",
    "\n",
    "                        (axs.flat[j]).plot(trackshist.index, gaussian_fitx, color=color, linewidth=0.3, alpha=0.3)\n",
    "                        (axsy.flat[j]).plot(trackshisty.index, gaussian_fity, color=color, linewidth=0.3, alpha=0.3)\n",
    "\n",
    "                        try:\n",
    "                            (axs.flat[j]).set_xlim(xlimx[j])\n",
    "                            (axs.flat[j]).set_ylim(ylimx[j])\n",
    "                            (axsy.flat[j]).set_xlim(xlimy[j])\n",
    "                            (axsy.flat[j]).set_ylim(ylimy[j])\n",
    "                        except:\n",
    "                            pass\n",
    "                    except:\n",
    "                        print('Failed to show Gaussian fits.')\n",
    "                    #ax4.scatter(x=vanhove_lagtime_insecs,y=gaussian_fit_paramsx[2], color = color, alpha = 0.3)\n",
    "\n",
    "                # Plot vanhove datapoints \n",
    "                trackshist.plot(\n",
    "                    marker=\".\",\n",
    "                    markersize=1,\n",
    "                    linestyle=\"\",\n",
    "                    ax=axs.flat[j],\n",
    "                    label=file + \" x\",\n",
    "                    color=color,\n",
    "                    alpha=0.3,\n",
    "                )\n",
    "\n",
    "                trackshisty.plot(\n",
    "                    marker=\".\",\n",
    "                    markersize=1,\n",
    "                    linestyle=\"\",\n",
    "                    ax=axsy.flat[j],\n",
    "                    label=file + \" y\",\n",
    "                    color=color,\n",
    "                    alpha=0.3,\n",
    "                )\n",
    "\n",
    "                for ax in [axs.flat[j], axsy.flat[j]]:\n",
    "                    ax.set_title(\"{:.2f} ms\".format(1000 * vanhove_lagtime_insecs))\n",
    "                    ax.set_xticks([int(trackshist.index.min())+1, 0, int(trackshist.index.max())])\n",
    "                    ax.set_yscale(\"log\")\n",
    "                if not show_curvefit:\n",
    "                    xlimx[j] = axs.flat[j].get_xlim()\n",
    "                    ylimx[j] = axs.flat[j].get_ylim()\n",
    "                    xlimy[j] = axsy.flat[j].get_xlim()\n",
    "                    ylimy[j] = axsy.flat[j].get_ylim()\n",
    "                j += 1\n",
    "        # end showprobdist\n",
    "\n",
    "        # Gaussian widths\n",
    "        ax4.plot(lagtimelist, np.abs(np.array(gaussian_fit_paramsxlist)[:, sigma_index]), color=color, alpha = .5)\n",
    "        ax4.plot(lagtimelist, np.abs(np.array(gaussian_fit_paramsylist)[:, sigma_index]), color=color, alpha = .5)\n",
    "\n",
    "        # Kurtosis\n",
    "        kurtosisdf.set_index(\"time (s)\").plot(ax=ax2, color=color, label=file + \" x\", alpha = .3)\n",
    "        kurtosisdfy.set_index(\"time (s)\").plot(ax=ax2, color=color, label=file + \" y\", alpha = .3)\n",
    "\n",
    "    # Consolidate the figure legend to just the unique filenames (not counting (1), (2), (3))\n",
    "\n",
    "    handles, legend_labels = ax2.get_legend_handles_labels()\n",
    "    short_labels = []\n",
    "    handle_mapping = {}\n",
    "    for handle, label in zip(handles, legend_labels):\n",
    "        match = re.search(r'(.+?) \\(\\d+\\) kurtosis', label)  # Extract the short file names using regular expressions\n",
    "        if match:\n",
    "            short_label = match.group(1)\n",
    "            if short_label not in handle_mapping:\n",
    "                handle_mapping[short_label] = handle\n",
    "                short_labels.append(short_label)\n",
    "        else:\n",
    "            if label not in handle_mapping:\n",
    "                handle_mapping[label] = handle\n",
    "                short_labels.append(label)\n",
    "\n",
    "    new_handles = [] # Create new handles\n",
    "    for label in short_labels:\n",
    "        handle = handle_mapping[label]\n",
    "        # Create new Line2D objects with solid lines and no alpha\n",
    "        new_handle = mlines.Line2D([], [], color=handle.get_color(), linestyle='-', linewidth=handle.get_linewidth())\n",
    "        new_handles.append(new_handle)\n",
    "\n",
    "    # Set the modified handles and labels\n",
    "    ax2.legend(handles=new_handles, labels=short_labels, bbox_to_anchor=(1, 1), loc='upper left')  # Move the legend to the right        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "warnings.resetwarnings()\n",
    "\n",
    "datestr = datestring()\n",
    "\n",
    "plt.sca(ax4)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Gaussian width (um)')\n",
    "plt.sca(ax2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel(\"Kurtosis\")\n",
    "\n",
    "if showprobdist:\n",
    "    fig3.tight_layout()\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "beep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa01099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saving = True\n",
    "\n",
    "if saving:\n",
    "    ## Save figures\n",
    "    if showprobdist:\n",
    "        plt.figure(fig)\n",
    "        savefigure(datestr + 'PDx')\n",
    "    plt.figure(fig2)\n",
    "    savefigure(datestr + 'kurtosis')\n",
    "    if showprobdist:\n",
    "        plt.figure(fig3)\n",
    "        savefigure(datestr + 'PDy')\n",
    "    plt.figure(fig4)\n",
    "    savefigure(datestr + 'Gaussian width')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658256d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71747b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "\n",
    "if 'kurtosisdflisty' in dir():\n",
    "    row = 2  # Select the desired row\n",
    "    shortnames = []  # List to store the modified short names\n",
    "    kurtosis_values = []  # List to store the kurtosis values\n",
    "\n",
    "    for kurtosisdf in kurtosisdflist + kurtosisdflisty:\n",
    "        shortname = kurtosisdf.columns[1]  # Extract the short name from the column name\n",
    "        shortname = re.sub(r'\\(\\d\\)\\s*kurtosis', '', shortname)  # Remove the \"(1) kurtosis\" or \"(2) kurtosis\" part\n",
    "        kurtosis_value = kurtosisdf.loc[row][1]  # Extract the kurtosis value from the specified row\n",
    "\n",
    "        shortnames.append(shortname)\n",
    "        kurtosis_values.append(kurtosis_value)\n",
    "\n",
    "    # Create plot\n",
    "    plt.scatter(shortnames, kurtosis_values)\n",
    "    plt.xlabel('Short Names')\n",
    "    plt.ylabel('Kurtosis')\n",
    "    plt.title('Kurtosis Values at Time ' + str(kurtosisdflist[0].loc[row][0]) + ' s')  # Use the time from the first DataFrame\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels if needed\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    assert len(big_data_df.Description) == len(big_data_df['kurtosis'])\n",
    "    \n",
    "    timechoice = big_data_df['time (s)'].unique()[1]\n",
    "    \n",
    "    \n",
    "    big_data_df_time_slice = big_data_df[big_data_df['time (s)'] == timechoice ]\n",
    "    \n",
    "    plt.scatter(big_data_df_time_slice.Description, big_data_df_time_slice['kurtosis'])\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels if needed\n",
    "\n",
    "    plt.title('Kurtosis Values at Time ' + str(timechoice) + ' s')  # Use the time from the first DataFrame\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6985695",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'kurtosisdflisty' in dir() and kurtosis_values in dir():\n",
    "    # Combine the shortnames and kurtosis_values into a list of tuples\n",
    "    data = list(zip(shortnames, kurtosis_values))\n",
    "\n",
    "    # Sort the list of tuples based on the kurtosis values\n",
    "    sorted_data = sorted(data, key=lambda x: x[1])  # Sort by the second element of each tuple\n",
    "\n",
    "    # Unzip the sorted list of tuples back into separate lists\n",
    "    shortnames, kurtosis_values = zip(*sorted_data)\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.scatter(shortnames, kurtosis_values)\n",
    "    #plt.xlabel('Short Names')\n",
    "    plt.ylabel('Kurtosis')\n",
    "    plt.title('Kurtosis Values at Time ' + str(kurtosisdflist[0].loc[row][0]) + ' s')  # Use the time from the first DataFrame\n",
    "    plt.xticks(rotation=90)  # Rotate x-axis labels if needed\n",
    "    plt.show()\n",
    "\n",
    "    # Create a violin plot\n",
    "    data_df = pd.DataFrame({'Short Names': shortnames, 'Kurtosis': kurtosis_values})\n",
    "\n",
    "    sns.violinplot(x='Short Names', y='Kurtosis', data=data_df, color='skyblue')\n",
    "    sns.stripplot(x='Short Names', y='Kurtosis', data=data_df, color='black', jitter=True, size=5)\n",
    "\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Kurtosis')\n",
    "    plt.title('Kurtosis Values at Time ' + str(kurtosisdf.loc[row][0]) + ' s')\n",
    "    plt.xticks(rotation=90);\n",
    "\n",
    "    os.chdir(savefolder)\n",
    "    saving = False\n",
    "    if saving:\n",
    "        datestr = datestring()\n",
    "        savefigure(datestr + 'kurtosis violin plot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c6d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 2  # Select the desired row\n",
    "\n",
    "shortnamesx = []  # List to store the modified short names\n",
    "shortnamesy=[]\n",
    "kurtosis_xvalues = []  # List to store the kurtosis values\n",
    "kurtosis_yvalues = []  # List to store the kurtosis values\n",
    "\n",
    "for kurtosisdf in kurtosisdflist:\n",
    "    if False:\n",
    "        shortname = kurtosisdf.columns[1]  # Extract the short name from the column name\n",
    "        shortname = re.sub(r'\\(\\d\\)\\s*kurtosis', '', shortname)  # Remove the \"(1) kurtosis\" or \"(2) kurtosis\" part\n",
    "        shortnamesx.append(shortname)\n",
    "    else:\n",
    "        shortnamesx.append(kurtosisdf['Description'][0])\n",
    "    \n",
    "    kurtosis_value = kurtosisdf.reset_index(drop=True).loc[row]['kurtosis']\n",
    "    \n",
    "    kurtosis_xvalues.append(kurtosis_value)\n",
    "    \n",
    "if 'kurtosisdflisty' in dir():\n",
    "    for kurtosisdf in kurtosisdflisty:\n",
    "        shortname = kurtosisdf.columns[1]  # Extract the short name from the column name\n",
    "        shortname = re.sub(r'\\(\\d\\)\\s*kurtosis', '', shortname)  # Remove the \"(1) kurtosis\" or \"(2) kurtosis\" part\n",
    "        kurtosis_value = kurtosisdf.loc[row][1]  # Extract the kurtosis value from the specified row\n",
    "\n",
    "        shortnamesy.append(shortname)\n",
    "        kurtosis_yvalues.append(kurtosis_value)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.scatter(shortnamesx, kurtosis_xvalues)\n",
    "plt.scatter(shortnamesy, kurtosis_yvalues)\n",
    "#plt.xlabel('Short Names')\n",
    "plt.ylabel('Kurtosis')\n",
    "plt.xticks(rotation=90);  # Rotate x-axis labels"
    "plt.title('Kurtosis Values at Time ' + str(kurtosisdf.reset_index(drop=True).loc[row][0]) + ' s')\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b26b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
